travis_fold:start:system_info[0K[33;1mBuild system information[0m
Build language: java
Build group: stable
Build dist: trusty
Build id: ''
Job id: ''
[34m[1mBuild image provisioning date and time[0m
Tue Dec  5 20:11:19 UTC 2017
[34m[1mOperating System Details[0m
Distributor ID:	Ubuntu
Description:	Ubuntu 14.04.5 LTS
Release:	14.04
Codename:	trusty
[34m[1mCookbooks Version[0m
7c2c6a6 https://github.com/travis-ci/travis-cookbooks/tree/7c2c6a6
[34m[1mgit version[0m
git version 2.15.1
[34m[1mbash version[0m
GNU bash, version 4.3.11(1)-release (x86_64-pc-linux-gnu)
[34m[1mgcc version[0m
gcc (Ubuntu 4.8.4-2ubuntu1~14.04.3) 4.8.4
Copyright (C) 2013 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

[34m[1mdocker version[0m
Client:
 Version:      17.09.0-ce
 API version:  1.32
 Go version:   go1.8.3
 Git commit:   afdb6d4
 Built:        Tue Sep 26 22:39:28 2017
 OS/Arch:      linux/amd64
[34m[1mclang version[0m
clang version 5.0.0 (tags/RELEASE_500/final)
Target: x86_64-unknown-linux-gnu
Thread model: posix
InstalledDir: /usr/local/clang-5.0.0/bin
[34m[1mjq version[0m
jq-1.5
[34m[1mbats version[0m
Bats 0.4.0
[34m[1mshellcheck version[0m
0.4.6
[34m[1mshfmt version[0m
v2.0.0
[34m[1mccache version[0m
ccache version 3.1.9

Copyright (C) 2002-2007 Andrew Tridgell
Copyright (C) 2009-2011 Joel Rosdahl

This program is free software; you can redistribute it and/or modify it under
the terms of the GNU General Public License as published by the Free Software
Foundation; either version 3 of the License, or (at your option) any later
version.
[34m[1mcmake version[0m
cmake version 3.9.2

CMake suite maintained and supported by Kitware (kitware.com/cmake).
[34m[1mheroku version[0m
heroku-cli/6.14.39-addc925 (linux-x64) node-v9.2.0
[34m[1mimagemagick version[0m
Version: ImageMagick 6.7.7-10 2017-07-31 Q16 http://www.imagemagick.org
[34m[1mmd5deep version[0m
4.2
[34m[1mmercurial version[0m
Mercurial Distributed SCM (version 4.2.2)
(see https://mercurial-scm.org for more information)

Copyright (C) 2005-2017 Matt Mackall and others
This is free software; see the source for copying conditions. There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
[34m[1mmysql version[0m
mysql  Ver 14.14 Distrib 5.6.33, for debian-linux-gnu (x86_64) using  EditLine wrapper
[34m[1mopenssl version[0m
OpenSSL 1.0.1f 6 Jan 2014
[34m[1mpacker version[0m
Packer v1.0.2

Your version of Packer is out of date! The latest version
is 1.1.2. You can update by downloading from www.packer.io
[34m[1mpostgresql client version[0m
psql (PostgreSQL) 9.6.6
[34m[1mragel version[0m
Ragel State Machine Compiler version 6.8 Feb 2013
Copyright (c) 2001-2009 by Adrian Thurston
[34m[1msubversion version[0m
svn, version 1.8.8 (r1568071)
   compiled Aug 10 2017, 17:20:39 on x86_64-pc-linux-gnu

Copyright (C) 2013 The Apache Software Foundation.
This software consists of contributions made by many people;
see the NOTICE file for more information.
Subversion is open source software, see http://subversion.apache.org/

The following repository access (RA) modules are available:

* ra_svn : Module for accessing a repository using the svn network protocol.
  - with Cyrus SASL authentication
  - handles 'svn' scheme
* ra_local : Module for accessing a repository on local disk.
  - handles 'file' scheme
* ra_serf : Module for accessing a repository via WebDAV protocol using serf.
  - using serf 1.3.3
  - handles 'http' scheme
  - handles 'https' scheme

[34m[1msudo version[0m
Sudo version 1.8.9p5
Configure options: --prefix=/usr -v --with-all-insults --with-pam --with-fqdn --with-logging=syslog --with-logfac=authpriv --with-env-editor --with-editor=/usr/bin/editor --with-timeout=15 --with-password-timeout=0 --with-passprompt=[sudo] password for %p:  --without-lecture --with-tty-tickets --disable-root-mailer --enable-admin-flag --with-sendmail=/usr/sbin/sendmail --with-timedir=/var/lib/sudo --mandir=/usr/share/man --libexecdir=/usr/lib/sudo --with-sssd --with-sssd-lib=/usr/lib/x86_64-linux-gnu --with-selinux
Sudoers policy plugin version 1.8.9p5
Sudoers file grammar version 43

Sudoers path: /etc/sudoers
Authentication methods: 'pam'
Syslog facility if syslog is being used for logging: authpriv
Syslog priority to use when user authenticates successfully: notice
Syslog priority to use when user authenticates unsuccessfully: alert
Send mail if the user is not in sudoers
Use a separate timestamp for each user/tty combo
Lecture user the first time they run sudo
Root may run sudo
Allow some information gathering to give useful error messages
Require fully-qualified hostnames in the sudoers file
Visudo will honor the EDITOR environment variable
Set the LOGNAME and USER environment variables
Length at which to wrap log file lines (0 for no wrap): 80
Authentication timestamp timeout: 15.0 minutes
Password prompt timeout: 0.0 minutes
Number of tries to enter a password: 3
Umask to use or 0777 to use user's: 022
Path to mail program: /usr/sbin/sendmail
Flags for mail program: -t
Address to send mail to: root
Subject line for mail messages: *** SECURITY information for %h ***
Incorrect password message: Sorry, try again.
Path to authentication timestamp dir: /var/lib/sudo
Default password prompt: [sudo] password for %p: 
Default user to run commands as: root
Value to override user's $PATH with: /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin
Path to the editor for use by visudo: /usr/bin/editor
When to require a password for 'list' pseudocommand: any
When to require a password for 'verify' pseudocommand: all
File descriptors >= 3 will be closed before executing a command
Environment variables to check for sanity:
	TZ
	TERM
	LINGUAS
	LC_*
	LANGUAGE
	LANG
	COLORTERM
Environment variables to remove:
	RUBYOPT
	RUBYLIB
	PYTHONUSERBASE
	PYTHONINSPECT
	PYTHONPATH
	PYTHONHOME
	TMPPREFIX
	ZDOTDIR
	READNULLCMD
	NULLCMD
	FPATH
	PERL5DB
	PERL5OPT
	PERL5LIB
	PERLLIB
	PERLIO_DEBUG 
	JAVA_TOOL_OPTIONS
	SHELLOPTS
	GLOBIGNORE
	PS4
	BASH_ENV
	ENV
	TERMCAP
	TERMPATH
	TERMINFO_DIRS
	TERMINFO
	_RLD*
	LD_*
	PATH_LOCALE
	NLSPATH
	HOSTALIASES
	RES_OPTIONS
	LOCALDOMAIN
	CDPATH
	IFS
Environment variables to preserve:
	JAVA_HOME
	TRAVIS
	CI
	DEBIAN_FRONTEND
	XAUTHORIZATION
	XAUTHORITY
	PS2
	PS1
	PATH
	LS_COLORS
	KRB5CCNAME
	HOSTNAME
	HOME
	DISPLAY
	COLORS
Locale to use while parsing sudoers: C
Directory in which to store input/output logs: /var/log/sudo-io
File in which to store the input/output log: %{seq}
Add an entry to the utmp/utmpx file when allocating a pty
PAM service name to use
PAM service name to use for login shells
Create a new PAM session for the command to run in
Maximum I/O log sequence number: 0

Local IP address and netmask pairs:
	172.17.0.2/255.255.0.0

Sudoers I/O plugin version 1.8.9p5
[34m[1mgzip version[0m
gzip 1.6
Copyright (C) 2007, 2010, 2011 Free Software Foundation, Inc.
Copyright (C) 1993 Jean-loup Gailly.
This is free software.  You may redistribute copies of it under the terms of
the GNU General Public License <http://www.gnu.org/licenses/gpl.html>.
There is NO WARRANTY, to the extent permitted by law.

Written by Jean-loup Gailly.
[34m[1mzip version[0m
Copyright (c) 1990-2008 Info-ZIP - Type 'zip "-L"' for software license.
This is Zip 3.0 (July 5th 2008), by Info-ZIP.
Currently maintained by E. Gordon.  Please send bug reports to
the authors using the web page at www.info-zip.org; see README for details.

Latest sources and executables are at ftp://ftp.info-zip.org/pub/infozip,
as of above date; see http://www.info-zip.org/ for other sites.

Compiled with gcc 4.8.2 for Unix (Linux ELF) on Oct 21 2013.

Zip special compilation options:
	USE_EF_UT_TIME       (store Universal Time)
	BZIP2_SUPPORT        (bzip2 library version 1.0.6, 6-Sept-2010)
	    bzip2 code and library copyright (c) Julian R Seward
	    (See the bzip2 license for terms of use)
	SYMLINK_SUPPORT      (symbolic links supported)
	LARGE_FILE_SUPPORT   (can read and write large files on file system)
	ZIP64_SUPPORT        (use Zip64 to store large files in archives)
	UNICODE_SUPPORT      (store and read UTF-8 Unicode paths)
	STORE_UNIX_UIDs_GIDs (store UID/GID sizes/values using new extra field)
	UIDGID_NOT_16BIT     (old Unix 16-bit UID/GID extra field not used)
	[encryption, version 2.91 of 05 Jan 2007] (modified for Zip 3)

Encryption notice:
	The encryption code of this program is not copyrighted and is
	put in the public domain.  It was originally written in Europe
	and, to the best of our knowledge, can be freely distributed
	in both source and object forms from any country, including
	the USA under License Exception TSU of the U.S. Export
	Administration Regulations (section 740.13(e)) of 6 June 2002.

Zip environment options:
             ZIP:  [none]
          ZIPOPT:  [none]
[34m[1mvim version[0m
VIM - Vi IMproved 7.4 (2013 Aug 10, compiled Nov 24 2016 16:43:18)
Included patches: 1-52
Extra patches: 8.0.0056
Modified by pkg-vim-maintainers@lists.alioth.debian.org
Compiled by buildd@
Huge version without GUI.  Features included (+) or not (-):
+acl             +farsi           +mouse_netterm   +syntax
+arabic          +file_in_path    +mouse_sgr       +tag_binary
+autocmd         +find_in_path    -mouse_sysmouse  +tag_old_static
-balloon_eval    +float           +mouse_urxvt     -tag_any_white
-browse          +folding         +mouse_xterm     -tcl
++builtin_terms  -footer          +multi_byte      +terminfo
+byte_offset     +fork()          +multi_lang      +termresponse
+cindent         +gettext         -mzscheme        +textobjects
-clientserver    -hangul_input    +netbeans_intg   +title
-clipboard       +iconv           +path_extra      -toolbar
+cmdline_compl   +insert_expand   -perl            +user_commands
+cmdline_hist    +jumplist        +persistent_undo +vertsplit
+cmdline_info    +keymap          +postscript      +virtualedit
+comments        +langmap         +printer         +visual
+conceal         +libcall         +profile         +visualextra
+cryptv          +linebreak       +python          +viminfo
+cscope          +lispindent      -python3         +vreplace
+cursorbind      +listcmds        +quickfix        +wildignore
+cursorshape     +localmap        +reltime         +wildmenu
+dialog_con      -lua             +rightleft       +windows
+diff            +menu            -ruby            +writebackup
+digraphs        +mksession       +scrollbind      -X11
-dnd             +modify_fname    +signs           -xfontset
-ebcdic          +mouse           +smartindent     -xim
+emacs_tags      -mouseshape      -sniff           -xsmp
+eval            +mouse_dec       +startuptime     -xterm_clipboard
+ex_extra        +mouse_gpm       +statusline      -xterm_save
+extra_search    -mouse_jsbterm   -sun_workshop    -xpm
   system vimrc file: "$VIM/vimrc"
     user vimrc file: "$HOME/.vimrc"
 2nd user vimrc file: "~/.vim/vimrc"
      user exrc file: "$HOME/.exrc"
  fall-back for $VIM: "/usr/share/vim"
Compilation: gcc -c -I. -Iproto -DHAVE_CONFIG_H     -g -O2 -fstack-protector --param=ssp-buffer-size=4 -Wformat -Werror=format-security -U_FORTIFY_SOURCE -D_FORTIFY_SOURCE=1      
Linking: gcc   -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,--as-needed -o vim        -lm -ltinfo -lnsl  -lselinux  -lacl -lattr -lgpm -ldl    -L/usr/lib/python2.7/config-x86_64-linux-gnu -lpython2.7 -lpthread -ldl -lutil -lm -Xlinker -export-dynamic -Wl,-O1 -Wl,-Bsymbolic-functions      
[34m[1miptables version[0m
iptables v1.4.21
[34m[1mcurl version[0m
curl 7.35.0 (x86_64-pc-linux-gnu) libcurl/7.35.0 OpenSSL/1.0.1f zlib/1.2.8 libidn/1.28 librtmp/2.3
[34m[1mwget version[0m
GNU Wget 1.15 built on linux-gnu.
[34m[1mrsync version[0m
rsync  version 3.1.0  protocol version 31
[34m[1mgimme version[0m
v1.2.0
[34m[1mnvm version[0m
0.33.6
[34m[1mperlbrew version[0m
/home/travis/perl5/perlbrew/bin/perlbrew  - App::perlbrew/0.80
[34m[1mphpenv version[0m
rbenv 1.1.1-25-g6aa70b6
[34m[1mrvm version[0m
rvm 1.29.3 (latest) by Michal Papis, Piotr Kuczynski, Wayne E. Seguin [https://rvm.io]
[34m[1mdefault ruby version[0m
ruby 2.4.1p111 (2017-03-22 revision 58053) [x86_64-linux]
[34m[1mCouchDB version[0m
couchdb 1.6.1
[34m[1mElasticSearch version[0m
5.5.0
[34m[1mInstalled Firefox version[0m
firefox 56.0.2
[34m[1mMongoDB version[0m
MongoDB 3.4.10
[34m[1mPhantomJS version[0m
2.1.1
[34m[1mPre-installed PostgreSQL versions[0m
9.2.24
9.3.20
9.4.15
9.5.10
9.6.6
[34m[1mRabbitMQ Version[0m
3.6.14
[34m[1mRedis version[0m
redis-server 4.0.6
[34m[1mriak version[0m
2.2.3
[34m[1mPre-installed Go versions[0m
1.7.4
[34m[1mant version[0m
Apache Ant(TM) version 1.9.3 compiled on April 8 2014
[34m[1mmvn version[0m
Apache Maven 3.5.2 (138edd61fd100ec658bfa2d307c43b76940a5d7d; 2017-10-18T07:58:13Z)
Maven home: /usr/local/maven-3.5.2
Java version: 1.8.0_151, vendor: Oracle Corporation
Java home: /usr/lib/jvm/java-8-oracle/jre
Default locale: en_US, platform encoding: UTF-8
OS name: "linux", version: "4.4.0-101-generic", arch: "amd64", family: "unix"
[34m[1mgradle version[0m

------------------------------------------------------------
Gradle 4.0.1
------------------------------------------------------------

Build time:   2017-07-07 14:02:41 UTC
Revision:     38e5dc0f772daecca1d2681885d3d85414eb6826

Groovy:       2.4.11
Ant:          Apache Ant(TM) version 1.9.6 compiled on June 29 2015
JVM:          1.8.0_151 (Oracle Corporation 25.151-b12)
OS:           Linux 4.4.0-101-generic amd64

[34m[1mlein version[0m
Leiningen 2.8.1 on Java 1.8.0_151 Java HotSpot(TM) 64-Bit Server VM
[34m[1mPre-installed Node.js versions[0m
v4.8.6
v6.12.0
v6.12.1
v8.9
v8.9.1
[34m[1mphpenv versions[0m
  system
  5.6
* 5.6.32 (set by /home/travis/.phpenv/version)
  7.0
  7.0.25
  7.1
  7.1.11
  hhvm
  hhvm-stable
[34m[1mcomposer --version[0m
Composer version 1.5.2 2017-09-11 16:59:25
[34m[1mPre-installed Ruby versions[0m
ruby-2.2.7
ruby-2.3.4
ruby-2.4.1
travis_fold:end:system_info[0K
W: GPG error: http://dl.google.com/linux/chrome/deb stable InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 78BD65473CB3BD13
W: The repository 'http://dl.google.com/linux/chrome/deb stable InRelease' is not signed.
W: There is no public key available for the following key IDs:
78BD65473CB3BD13  
W: The repository 'http://www.apache.org/dist/cassandra/debian 39x Release' does not have a Release file.
W: The repository 'http://apt.postgresql.org/pub/repos/apt trusty-pgdg Release' does not have a Release file.
W: GPG error: http://repo.mongodb.org/apt/ubuntu trusty/mongodb-org/3.4 Release: The following signatures were invalid: KEYEXPIRED 1515625755
W: The repository 'http://repo.mongodb.org/apt/ubuntu trusty/mongodb-org/3.4 Release' is not signed.
W: GPG error: https://packagecloud.io/github/git-lfs/ubuntu trusty InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 6B05F25D762E3157
W: The repository 'https://packagecloud.io/github/git-lfs/ubuntu trusty InRelease' is not signed.
W: There is no public key available for the following key IDs:
6B05F25D762E3157  
W: The repository 'https://packagecloud.io/basho/riak/ubuntu trusty Release' does not have a Release file.
W: GPG error: https://packagecloud.io/rabbitmq/rabbitmq-server/ubuntu trusty InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY F6609E60DC62814E
W: The repository 'https://packagecloud.io/rabbitmq/rabbitmq-server/ubuntu trusty InRelease' is not signed.
W: There is no public key available for the following key IDs:
F6609E60DC62814E  
W: http://ppa.launchpad.net/couchdb/stable/ubuntu/dists/trusty/Release.gpg: Signature by key 15866BAFD9BCC4F3C1E0DFC7D69548E1C17EAB57 uses weak digest algorithm (SHA1)
E: Failed to fetch https://www.apache.org/dist/cassandra/debian/dists/39x/main/binary-amd64/Packages  gnutls_handshake() failed: Handshake failed
E: Failed to fetch https://www.apache.org/dist/cassandra/debian/dists/39x/main/binary-i386/Packages  gnutls_handshake() failed: Handshake failed
E: Failed to fetch http://apt.postgresql.org/pub/repos/apt/dists/trusty-pgdg/main/binary-amd64/Packages  404  Not Found [IP: 87.238.57.227 80]
E: Failed to fetch http://apt.postgresql.org/pub/repos/apt/dists/trusty-pgdg/main/binary-i386/Packages  404  Not Found [IP: 87.238.57.227 80]
E: Failed to fetch https://packagecloud.io/basho/riak/ubuntu/dists/trusty/main/source/Sources  HttpError402
E: Failed to fetch https://packagecloud.io/basho/riak/ubuntu/dists/trusty/main/binary-amd64/Packages  HttpError402
E: Failed to fetch https://packagecloud.io/basho/riak/ubuntu/dists/trusty/main/binary-i386/Packages  HttpError402
E: Some index files failed to download. They have been ignored, or old ones used instead.
sed: cannot rename /etc/hosts: Device or resource busy
sed: cannot rename /etc/hosts: Device or resource busy
$ jdk_switcher use openjdk7
Switching to OpenJDK7 (java-1.7.0-openjdk-amd64), JAVA_HOME will be set to /usr/lib/jvm/java-7-openjdk-amd64
$ cd failed/crate/crate-jdbc
travis_fold:start:git.submodule[0Ktravis_time:start:06afacd6[0K$ git submodule update --init --recursive

travis_time:end:06afacd6:start=1645345517973347374,finish=1645345520743665177,duration=2770317803[0Ktravis_fold:end:git.submodule[0K
[33;1mSetting environment variables from .travis.yml[0m
$ export CRATE_VERSION=0.57.6

$ export TERM=dumb
travis_fold:start:cache.1[0KSetting up build cache
$ export CASHER_DIR=$HOME/.casher
travis_time:start:01fa6c6a[0K
travis_time:end:01fa6c6a:start=1645345530457773789,finish=1645345530468053968,duration=10280179[0Ktravis_time:start:1e19e522[0K/home/travis/.casher/bin/casher:213:in `cache_archive_name': undefined method `[]' for nil:NilClass (NoMethodError)
	from /home/travis/.casher/bin/casher:65:in `block in fetch'
	from /home/travis/.casher/bin/casher:64:in `each'
	from /home/travis/.casher/bin/casher:64:in `fetch'
	from /home/travis/.casher/bin/casher:53:in `block in run'
	from /home/travis/.rvm/rubies/ruby-2.2.7/lib/ruby/2.2.0/timeout.rb:88:in `block in timeout'
	from /home/travis/.rvm/rubies/ruby-2.2.7/lib/ruby/2.2.0/timeout.rb:32:in `block in catch'
	from /home/travis/.rvm/rubies/ruby-2.2.7/lib/ruby/2.2.0/timeout.rb:32:in `catch'
	from /home/travis/.rvm/rubies/ruby-2.2.7/lib/ruby/2.2.0/timeout.rb:32:in `catch'
	from /home/travis/.rvm/rubies/ruby-2.2.7/lib/ruby/2.2.0/timeout.rb:103:in `timeout'
	from /home/travis/.casher/bin/casher:53:in `run'
	from /home/travis/.casher/bin/casher:263:in `<main>'
[32;1mattempting to download cache archive[0m

travis_time:end:1e19e522:start=1645345530480223253,finish=1645345531385085416,duration=904862163[0Ktravis_time:start:1ff87e2a[0K
travis_time:end:1ff87e2a:start=1645345531398068158,finish=1645345531410675908,duration=12607750[0Ktravis_time:start:074c5d40[0K[32;1madding /home/travis/.m2 to cache[0m

travis_time:end:074c5d40:start=1645345531425092106,finish=1645345533748945137,duration=2323853031[0Ktravis_fold:end:cache.1[0K$ java -Xmx32m -version
Picked up _JAVA_OPTIONS: -Xmx2048m -Xms512m
java version "1.7.0_151"
OpenJDK Runtime Environment (IcedTea 2.6.11) (7u151-2.6.11-2ubuntu0.14.04.1)
OpenJDK 64-Bit Server VM (build 24.151-b01, mixed mode)
$ javac -J-Xmx32m -version
Picked up _JAVA_OPTIONS: -Xmx2048m -Xms512m
javac 1.7.0_151
travis_fold:start:install[0Ktravis_time:start:03be7270[0K$ JAVA_HOME=$(jdk_switcher home openjdk8) ./gradlew classes testClasses
Picked up _JAVA_OPTIONS: -Xmx2048m -Xms512m
Starting a Gradle Daemon, 2 incompatible and 1 stopped Daemons could not be reused, use --status for details
:pg:preprocessJava
Preproces: upstream/pgjdbc/src/main/java -> /home/travis/build/failed/crate/crate-jdbc/pg/build/jcp/main/java
Preproces: upstream/pgjdbc/src/main/resources -> /home/travis/build/failed/crate/crate-jdbc/pg/build/jcp/main/resources
:pg:compileJavaPicked up _JAVA_OPTIONS: -Xmx2048m -Xms512m
Note: Some input files use or override a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
Note: Some input files use unchecked or unsafe operations.
Note: Recompile with -Xlint:unchecked for details.

:pg:processResources UP-TO-DATE
:pg:classes
:pg:jar UP-TO-DATE
:compileJavawarning: [options] bootstrap class path not set in conjunction with -source 1.7
1 warning

:processResources UP-TO-DATE
:classes
:compileTestJavawarning: [options] bootstrap class path not set in conjunction with -source 1.7
1 warning

:processTestResources NO-SOURCE
:testClasses
:pg:compileTestJava NO-SOURCE
:pg:processTestResources NO-SOURCE
:pg:testClasses UP-TO-DATE

BUILD SUCCESSFUL in 4m 23s
7 actionable tasks: 4 executed, 3 up-to-date

travis_time:end:03be7270:start=1645345534957038718,finish=1645345799160063635,duration=264203024917[0Ktravis_fold:end:install[0Ktravis_time:start:313c4430[0K$ ./gradlew test -s
Picked up _JAVA_OPTIONS: -Xmx2048m -Xms512m
Starting a Gradle Daemon, 2 incompatible and 2 stopped Daemons could not be reused, use --status for details
:pg:preprocessJava
Preproces: upstream/pgjdbc/src/main/java -> /home/travis/build/failed/crate/crate-jdbc/pg/build/jcp/main/java
Preproces: upstream/pgjdbc/src/main/resources -> /home/travis/build/failed/crate/crate-jdbc/pg/build/jcp/main/resources
:pg:compileJavaPicked up _JAVA_OPTIONS: -Xmx2048m -Xms512m
Note: Some input files use or override a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
Note: Some input files use unchecked or unsafe operations.
Note: Recompile with -Xlint:unchecked for details.

:pg:processResources UP-TO-DATE
:pg:classes
:pg:jar UP-TO-DATE
:compileJava
:processResources UP-TO-DATE
:classes
:compileTestJava
:processTestResources NO-SOURCE
:testClasses
:testPicked up _JAVA_OPTIONS: -Xmx2048m -Xms512m


io.crate.client.jdbc.integrationtests.CrateJDBCByPassSpecSettingTest STANDARD_OUT
    No need to download crate. Already downloaded VERSION[0.57.6] to: /tmp/crate.testing/downloads/crate-0.57.6.tar.gz
    Starting crate server process...
Picked up _JAVA_OPTIONS: -Xmx2048m -Xms512m
[2022-02-20 08:30:43,953][WARN ][bootstrap                ] unable to install syscall filter: seccomp unavailable: your kernel is buggy and you should upgrade
[2022-02-20 08:30:44,122][INFO ][node                     ] [Hochgasser] version[2.4.2], pid[2795], build[${build/NA]
[2022-02-20 08:30:44,122][INFO ][node                     ] [Hochgasser] initializing ...
[2022-02-20 08:30:44,352][INFO ][io.crate.plugin          ] [Hochgasser] plugins loaded: [crate-sigar] 
[2022-02-20 08:30:45,069][INFO ][plugins                  ] [Hochgasser] modules [], plugins [blob, udc, crate-azure-discovery, crate-core, repository-hdfs, discovery-multicast, crate-admin-ui, cloud-aws, crate, discovery-srv, admin-ui], sites [crate-admin-ui]
[2022-02-20 08:30:45,094][INFO ][env                      ] [Hochgasser] using [1] data paths, mounts [[/ (overlay)]], net usable_space [84.5tb], net total_space [90.9tb], spins? [possibly], types [overlay]
[2022-02-20 08:30:45,094][INFO ][env                      ] [Hochgasser] heap size [1.9gb], compressed ordinary object pointers [true]
[2022-02-20 08:30:45,094][WARN ][env                      ] [Hochgasser] max file descriptors [30000] for elasticsearch process likely too low, consider increasing to at least [65536]
[2022-02-20 08:30:45,344][INFO ][http                     ] [Hochgasser] Using [io.crate.http.netty.CrateNettyHttpServerTransport] as http transport, overridden by [crate]
[2022-02-20 08:30:45,356][INFO ][plugin.hadoop.hdfs       ] Loaded Hadoop [Unknown] libraries from file:/tmp/crate.testing/working/crate-0.57.6_3270e193-91c9-4461-b357-90e1f81f11b5/plugins/elasticsearch-repository-hdfs/
[2022-02-20 08:30:45,406][INFO ][io.crate.module          ] [Hochgasser] configuring crate. version: 0.57.6
[2022-02-20 08:30:45,656][INFO ][io.crate.module          ] [Hochgasser] configuring crate. version: 0.57.6
[2022-02-20 08:30:47,095][INFO ][io.crate.operation.reference.sys.cluster.ClusterSettingsExpression$ApplySettings] updating [psql.port] from [5432-5532] to [5497]
[2022-02-20 08:30:47,871][INFO ][io.crate.rest            ] [Hochgasser] Elasticsearch HTTP REST API not enabled
[2022-02-20 08:30:47,939][INFO ][node                     ] [Hochgasser] initialized
[2022-02-20 08:30:47,940][INFO ][node                     ] [Hochgasser] starting ...
[2022-02-20 08:30:48,131][INFO ][psql                     ] [Hochgasser] publish_address {127.0.0.1:5497}, bound_addresses {127.0.0.1:5497}
[2022-02-20 08:30:48,131][INFO ][io.crate.blob.BlobService] [Hochgasser] BlobService.doStart() io.crate.blob.BlobService@3106b736
[2022-02-20 08:30:48,200][INFO ][http                     ] [Hochgasser] publish_address {127.0.0.1:4598}, bound_addresses {127.0.0.1:4598}
[2022-02-20 08:30:48,254][INFO ][transport                ] [Hochgasser] publish_address {127.0.0.1:4353}, bound_addresses {127.0.0.1:4353}
[2022-02-20 08:30:48,259][INFO ][discovery                ] [Hochgasser] TestingCluster/dCm3gN5bTZ6JLT9GYaBmkA
[2022-02-20 08:30:51,291][INFO ][cluster.service          ] [Hochgasser] new_master {Hochgasser}{dCm3gN5bTZ6JLT9GYaBmkA}{127.0.0.1}{127.0.0.1:4353}{http_address=127.0.0.1:4598, info.extended.type=sigar}, reason: zen-disco-join(elected_as_master, [0] joins received)
[2022-02-20 08:30:51,297][INFO ][node                     ] [Hochgasser] started
[2022-02-20 08:30:52,431][INFO ][gateway                  ] [Hochgasser] recovered [0] indices into cluster_state
    Stopping crate server process...
[2022-02-20 08:30:52,774][INFO ][node                     ] [Hochgasser] stopping ...
[2022-02-20 08:30:52,855][INFO ][node                     ] [Hochgasser] stopped
[2022-02-20 08:30:52,856][INFO ][node                     ] [Hochgasser] closing ...
[2022-02-20 08:30:52,866][INFO ][node                     ] [Hochgasser] closed

io.crate.client.jdbc.integrationtests.CrateJDBCConnectionTest STANDARD_OUT
    No need to download crate. Already downloaded VERSION[0.57.6] to: /tmp/crate.testing/downloads/crate-0.57.6.tar.gz
    Starting crate server process...
Picked up _JAVA_OPTIONS: -Xmx2048m -Xms512m
[2022-02-20 08:30:54,531][WARN ][bootstrap                ] unable to install syscall filter: seccomp unavailable: your kernel is buggy and you should upgrade
[2022-02-20 08:30:54,725][INFO ][node                     ] [Hocharn] version[2.4.2], pid[3142], build[${build/NA]
[2022-02-20 08:30:54,726][INFO ][node                     ] [Hocharn] initializing ...
[2022-02-20 08:30:54,987][INFO ][io.crate.plugin          ] [Hocharn] plugins loaded: [crate-sigar] 
[2022-02-20 08:30:55,999][INFO ][plugins                  ] [Hocharn] modules [], plugins [blob, udc, crate-azure-discovery, crate-core, repository-hdfs, discovery-multicast, crate-admin-ui, cloud-aws, crate, discovery-srv, admin-ui], sites [crate-admin-ui]
[2022-02-20 08:30:56,027][INFO ][env                      ] [Hocharn] using [1] data paths, mounts [[/ (overlay)]], net usable_space [84.5tb], net total_space [90.9tb], spins? [possibly], types [overlay]
[2022-02-20 08:30:56,027][INFO ][env                      ] [Hocharn] heap size [1.9gb], compressed ordinary object pointers [true]
[2022-02-20 08:30:56,027][WARN ][env                      ] [Hocharn] max file descriptors [30000] for elasticsearch process likely too low, consider increasing to at least [65536]
[2022-02-20 08:30:56,350][INFO ][http                     ] [Hocharn] Using [io.crate.http.netty.CrateNettyHttpServerTransport] as http transport, overridden by [crate]
[2022-02-20 08:30:56,367][INFO ][plugin.hadoop.hdfs       ] Loaded Hadoop [Unknown] libraries from file:/tmp/crate.testing/working/crate-0.57.6_3d22725e-1ebb-4894-ad5f-a4d7288b3f46/plugins/elasticsearch-repository-hdfs/
[2022-02-20 08:30:56,445][INFO ][io.crate.module          ] [Hocharn] configuring crate. version: 0.57.6
[2022-02-20 08:30:56,745][INFO ][io.crate.module          ] [Hocharn] configuring crate. version: 0.57.6
[2022-02-20 08:30:58,162][INFO ][io.crate.operation.reference.sys.cluster.ClusterSettingsExpression$ApplySettings] updating [psql.port] from [5432-5532] to [5433]
[2022-02-20 08:30:58,994][INFO ][io.crate.rest            ] [Hocharn] Elasticsearch HTTP REST API not enabled
[2022-02-20 08:30:59,079][INFO ][node                     ] [Hocharn] initialized
[2022-02-20 08:30:59,079][INFO ][node                     ] [Hocharn] starting ...
[2022-02-20 08:30:59,345][INFO ][psql                     ] [Hocharn] publish_address {127.0.0.1:5433}, bound_addresses {127.0.0.1:5433}
[2022-02-20 08:30:59,345][INFO ][io.crate.blob.BlobService] [Hocharn] BlobService.doStart() io.crate.blob.BlobService@7bfe5f3c
[2022-02-20 08:30:59,461][INFO ][http                     ] [Hocharn] publish_address {127.0.0.1:4534}, bound_addresses {127.0.0.1:4534}
[2022-02-20 08:30:59,586][INFO ][transport                ] [Hocharn] publish_address {127.0.0.1:4277}, bound_addresses {127.0.0.1:4277}
[2022-02-20 08:30:59,592][INFO ][discovery                ] [Hocharn] TestingCluster/Rl3GOGWRRCSB-Mp2S3bBHw
[2022-02-20 08:31:00,719][DEBUG][action.admin.indices.create] [Hocharn] no known master node, scheduling a retry
[2022-02-20 08:31:02,625][INFO ][cluster.service          ] [Hocharn] new_master {Hocharn}{Rl3GOGWRRCSB-Mp2S3bBHw}{127.0.0.1}{127.0.0.1:4277}{http_address=127.0.0.1:4534, info.extended.type=sigar}, reason: zen-disco-join(elected_as_master, [0] joins received)
[2022-02-20 08:31:02,631][INFO ][node                     ] [Hocharn] started
[2022-02-20 08:31:03,795][INFO ][gateway                  ] [Hocharn] recovered [0] indices into cluster_state
[2022-02-20 08:31:04,094][INFO ][cluster.metadata         ] [Hocharn] [test] creating index, cause [api], templates [], shards [1]/[0], mappings [default]
[2022-02-20 08:31:07,687][INFO ][cluster.routing.allocation] [Hocharn] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[test][0]] ...]).

io.crate.client.jdbc.integrationtests.CrateJDBCConnectionTest > testExecuteBatchPreparedStatementFailBulkTypes FAILED
    java.lang.AssertionError at CrateJDBCConnectionTest.java:284
[2022-02-20 08:31:10,612][DEBUG][action.bulk              ] create new coordinator for node Rl3GOGWRRCSB-Mp2S3bBHw and shard [test][0]

io.crate.client.jdbc.integrationtests.CrateJDBCConnectionTest STANDARD_OUT
    Stopping crate server process...
[2022-02-20 08:31:15,003][INFO ][node                     ] [Hocharn] stopping ...
[2022-02-20 08:31:18,631][INFO ][node                     ] [Hocharn] stopped
[2022-02-20 08:31:18,631][INFO ][node                     ] [Hocharn] closing ...
[2022-02-20 08:31:18,641][INFO ][node                     ] [Hocharn] closed

io.crate.client.jdbc.integrationtests.CrateJDBCDriverTest STANDARD_OUT
    No need to download crate. Already downloaded VERSION[0.57.6] to: /tmp/crate.testing/downloads/crate-0.57.6.tar.gz
    Starting crate server process...
Picked up _JAVA_OPTIONS: -Xmx2048m -Xms512m
[2022-02-20 08:31:20,236][WARN ][bootstrap                ] unable to install syscall filter: seccomp unavailable: your kernel is buggy and you should upgrade
[2022-02-20 08:31:20,433][INFO ][node                     ] [Soiernspitze] version[2.4.2], pid[3521], build[${build/NA]
[2022-02-20 08:31:20,433][INFO ][node                     ] [Soiernspitze] initializing ...
[2022-02-20 08:31:20,678][INFO ][io.crate.plugin          ] [Soiernspitze] plugins loaded: [crate-sigar] 
[2022-02-20 08:31:21,393][INFO ][plugins                  ] [Soiernspitze] modules [], plugins [blob, udc, crate-azure-discovery, crate-core, repository-hdfs, discovery-multicast, crate-admin-ui, cloud-aws, crate, discovery-srv, admin-ui], sites [crate-admin-ui]
[2022-02-20 08:31:21,415][INFO ][env                      ] [Soiernspitze] using [1] data paths, mounts [[/ (overlay)]], net usable_space [84.5tb], net total_space [90.9tb], spins? [possibly], types [overlay]
[2022-02-20 08:31:21,415][INFO ][env                      ] [Soiernspitze] heap size [1.9gb], compressed ordinary object pointers [true]
[2022-02-20 08:31:21,415][WARN ][env                      ] [Soiernspitze] max file descriptors [30000] for elasticsearch process likely too low, consider increasing to at least [65536]
[2022-02-20 08:31:21,683][INFO ][http                     ] [Soiernspitze] Using [io.crate.http.netty.CrateNettyHttpServerTransport] as http transport, overridden by [crate]
[2022-02-20 08:31:21,697][INFO ][plugin.hadoop.hdfs       ] Loaded Hadoop [Unknown] libraries from file:/tmp/crate.testing/working/crate-0.57.6_6630754d-1e3d-4b73-92e0-acc05105b879/plugins/elasticsearch-repository-hdfs/
[2022-02-20 08:31:21,764][INFO ][io.crate.module          ] [Soiernspitze] configuring crate. version: 0.57.6
[2022-02-20 08:31:22,012][INFO ][io.crate.module          ] [Soiernspitze] configuring crate. version: 0.57.6
[2022-02-20 08:31:23,494][INFO ][io.crate.operation.reference.sys.cluster.ClusterSettingsExpression$ApplySettings] updating [psql.port] from [5432-5532] to [5455]
[2022-02-20 08:31:24,321][INFO ][io.crate.rest            ] [Soiernspitze] Elasticsearch HTTP REST API not enabled
[2022-02-20 08:31:24,390][INFO ][node                     ] [Soiernspitze] initialized
[2022-02-20 08:31:24,390][INFO ][node                     ] [Soiernspitze] starting ...
[2022-02-20 08:31:24,569][INFO ][psql                     ] [Soiernspitze] publish_address {127.0.0.1:5455}, bound_addresses {127.0.0.1:5455}
[2022-02-20 08:31:24,569][INFO ][io.crate.blob.BlobService] [Soiernspitze] BlobService.doStart() io.crate.blob.BlobService@4090d8dd
[2022-02-20 08:31:24,637][INFO ][http                     ] [Soiernspitze] publish_address {127.0.0.1:4584}, bound_addresses {127.0.0.1:4584}
[2022-02-20 08:31:24,694][INFO ][transport                ] [Soiernspitze] publish_address {127.0.0.1:4254}, bound_addresses {127.0.0.1:4254}
[2022-02-20 08:31:24,702][INFO ][discovery                ] [Soiernspitze] TestingCluster/vO3o3zYTRGuzi1ztECgARw
    Stopping crate server process...
[2022-02-20 08:31:25,223][INFO ][node                     ] [Soiernspitze] stopping ...
[2022-02-20 08:31:25,279][INFO ][node                     ] [Soiernspitze] stopped
[2022-02-20 08:31:25,280][INFO ][node                     ] [Soiernspitze] closing ...
[2022-02-20 08:31:25,286][INFO ][node                     ] [Soiernspitze] closed

io.crate.client.jdbc.integrationtests.CrateJDBCFetchSizeIntegrationTest STANDARD_OUT
    No need to download crate. Already downloaded VERSION[0.57.6] to: /tmp/crate.testing/downloads/crate-0.57.6.tar.gz
    Starting crate server process...
Picked up _JAVA_OPTIONS: -Xmx2048m -Xms512m
[2022-02-20 08:31:26,794][WARN ][bootstrap                ] unable to install syscall filter: seccomp unavailable: your kernel is buggy and you should upgrade
[2022-02-20 08:31:26,969][INFO ][node                     ] [Plenge] version[2.4.2], pid[3865], build[${build/NA]
[2022-02-20 08:31:26,969][INFO ][node                     ] [Plenge] initializing ...
[2022-02-20 08:31:27,225][INFO ][io.crate.plugin          ] [Plenge] plugins loaded: [crate-sigar] 
[2022-02-20 08:31:28,048][INFO ][plugins                  ] [Plenge] modules [], plugins [blob, udc, crate-azure-discovery, crate-core, repository-hdfs, discovery-multicast, crate-admin-ui, cloud-aws, crate, discovery-srv, admin-ui], sites [crate-admin-ui]
[2022-02-20 08:31:28,071][INFO ][env                      ] [Plenge] using [1] data paths, mounts [[/ (overlay)]], net usable_space [84.5tb], net total_space [90.9tb], spins? [possibly], types [overlay]
[2022-02-20 08:31:28,071][INFO ][env                      ] [Plenge] heap size [1.9gb], compressed ordinary object pointers [true]
[2022-02-20 08:31:28,071][WARN ][env                      ] [Plenge] max file descriptors [30000] for elasticsearch process likely too low, consider increasing to at least [65536]
[2022-02-20 08:31:28,342][INFO ][http                     ] [Plenge] Using [io.crate.http.netty.CrateNettyHttpServerTransport] as http transport, overridden by [crate]
[2022-02-20 08:31:28,356][INFO ][plugin.hadoop.hdfs       ] Loaded Hadoop [Unknown] libraries from file:/tmp/crate.testing/working/crate-0.57.6_ceff15bc-d5dd-44d4-be94-ad38dca9aac9/plugins/elasticsearch-repository-hdfs/
[2022-02-20 08:31:28,432][INFO ][io.crate.module          ] [Plenge] configuring crate. version: 0.57.6
[2022-02-20 08:31:28,652][INFO ][io.crate.module          ] [Plenge] configuring crate. version: 0.57.6
[2022-02-20 08:31:30,113][INFO ][io.crate.operation.reference.sys.cluster.ClusterSettingsExpression$ApplySettings] updating [psql.port] from [5432-5532] to [5466]
[2022-02-20 08:31:30,887][INFO ][io.crate.rest            ] [Plenge] Elasticsearch HTTP REST API not enabled
[2022-02-20 08:31:30,948][INFO ][node                     ] [Plenge] initialized
[2022-02-20 08:31:30,948][INFO ][node                     ] [Plenge] starting ...
[2022-02-20 08:31:31,110][INFO ][psql                     ] [Plenge] publish_address {127.0.0.1:5466}, bound_addresses {127.0.0.1:5466}
[2022-02-20 08:31:31,110][INFO ][io.crate.blob.BlobService] [Plenge] BlobService.doStart() io.crate.blob.BlobService@732fd69c
[2022-02-20 08:31:31,166][INFO ][http                     ] [Plenge] publish_address {127.0.0.1:4536}, bound_addresses {127.0.0.1:4536}
[2022-02-20 08:31:31,220][INFO ][transport                ] [Plenge] publish_address {127.0.0.1:4312}, bound_addresses {127.0.0.1:4312}
[2022-02-20 08:31:31,228][INFO ][discovery                ] [Plenge] TestingCluster/8c4Zlg_OSemGUA-n_bJvlQ
    Stopping crate server process...
[2022-02-20 08:31:32,055][INFO ][node                     ] [Plenge] stopping ...
[2022-02-20 08:31:32,747][WARN ][discovery.zen.ping.unicast] [Plenge] failed to send ping to [{#zen_unicast_1#}{127.0.0.1}{127.0.0.1:4312}]
SendRequestTransportException[[Plenge][127.0.0.1:4312][internal:discovery/zen/unicast]]; nested: TransportException[TransportService is closed stopped can't send request];
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:336)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPingRequestToNode(UnicastZenPing.java:440)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPings(UnicastZenPing.java:426)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing$2.doRun(UnicastZenPing.java:249)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1152)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:622)
	at java.lang.Thread.run(Thread.java:748)
Caused by: TransportException[TransportService is closed stopped can't send request]
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:316)
	... 7 more
[2022-02-20 08:31:32,747][WARN ][discovery.zen.ping.unicast] [Plenge] failed to send ping to [{Plenge}{8c4Zlg_OSemGUA-n_bJvlQ}{127.0.0.1}{127.0.0.1:4312}{http_address=127.0.0.1:4536, info.extended.type=sigar}]
SendRequestTransportException[[Plenge][127.0.0.1:4312][internal:discovery/zen/unicast]]; nested: TransportException[TransportService is closed stopped can't send request];
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:336)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPingRequestToNode(UnicastZenPing.java:440)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPings(UnicastZenPing.java:426)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing$2.doRun(UnicastZenPing.java:249)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1152)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:622)
	at java.lang.Thread.run(Thread.java:748)
Caused by: TransportException[TransportService is closed stopped can't send request]
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:316)
	... 7 more
[2022-02-20 08:31:34,248][WARN ][discovery.zen.ping.unicast] [Plenge] failed to send ping to [{#zen_unicast_1#}{127.0.0.1}{127.0.0.1:4312}]
SendRequestTransportException[[Plenge][127.0.0.1:4312][internal:discovery/zen/unicast]]; nested: TransportException[TransportService is closed stopped can't send request];
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:336)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPingRequestToNode(UnicastZenPing.java:440)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPings(UnicastZenPing.java:426)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing$2$1.doRun(UnicastZenPing.java:253)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1152)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:622)
	at java.lang.Thread.run(Thread.java:748)
Caused by: TransportException[TransportService is closed stopped can't send request]
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:316)
	... 7 more
[2022-02-20 08:31:34,249][WARN ][discovery.zen.ping.unicast] [Plenge] failed to send ping to [{Plenge}{8c4Zlg_OSemGUA-n_bJvlQ}{127.0.0.1}{127.0.0.1:4312}{http_address=127.0.0.1:4536, info.extended.type=sigar}]
SendRequestTransportException[[Plenge][127.0.0.1:4312][internal:discovery/zen/unicast]]; nested: TransportException[TransportService is closed stopped can't send request];
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:336)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPingRequestToNode(UnicastZenPing.java:440)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPings(UnicastZenPing.java:426)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing$2$1.doRun(UnicastZenPing.java:253)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1152)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:622)
	at java.lang.Thread.run(Thread.java:748)
Caused by: TransportException[TransportService is closed stopped can't send request]
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:316)
	... 7 more
[2022-02-20 08:31:38,422][WARN ][netty.channel.DefaultChannelPipeline] An exception was thrown by an exception handler.
java.util.concurrent.RejectedExecutionException: Worker has already been shutdown
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.registerTask(AbstractNioSelector.java:120)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:72)
	at org.jboss.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:36)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:56)
	at org.jboss.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:36)
	at org.jboss.netty.channel.socket.nio.AbstractNioChannelSink.execute(AbstractNioChannelSink.java:34)
	at org.jboss.netty.channel.DefaultChannelPipeline.execute(DefaultChannelPipeline.java:636)
	at org.jboss.netty.channel.Channels.fireExceptionCaughtLater(Channels.java:496)
	at org.jboss.netty.channel.AbstractChannelSink.exceptionCaught(AbstractChannelSink.java:46)
	at org.jboss.netty.channel.DefaultChannelPipeline.notifyHandlerException(DefaultChannelPipeline.java:658)
	at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:577)
	at org.jboss.netty.channel.Channels.write(Channels.java:704)
	at org.jboss.netty.channel.Channels.write(Channels.java:671)
	at org.jboss.netty.channel.AbstractChannel.write(AbstractChannel.java:347)
	at io.crate.protocols.postgres.Messages.sendErrorResponse(Messages.java:259)
	at io.crate.protocols.postgres.ResultSetReceiver.fail(ResultSetReceiver.java:76)
	at io.crate.protocols.postgres.SimplePortal$ResultReceiverRetryWrapper.fail(SimplePortal.java:274)
	at io.crate.action.sql.RowReceiverToResultReceiver.fail(RowReceiverToResultReceiver.java:67)
	at io.crate.executor.transport.executionphases.InterceptingRowReceiver$1.onFailure(InterceptingRowReceiver.java:129)
	at io.crate.executor.MultiActionListener.countdown(MultiActionListener.java:68)
	at io.crate.executor.MultiActionListener.onFailure(MultiActionListener.java:59)
	at io.crate.executor.transport.DefaultTransportResponseHandler.handleException(DefaultTransportResponseHandler.java:62)
	at org.elasticsearch.transport.TransportService$3.doRun(TransportService.java:349)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1152)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:622)
	at java.lang.Thread.run(Thread.java:748)
[2022-02-20 08:31:38,424][INFO ][node                     ] [Plenge] stopped
[2022-02-20 08:31:38,424][INFO ][node                     ] [Plenge] closing ...
[2022-02-20 08:31:38,432][INFO ][node                     ] [Plenge] closed

io.crate.client.jdbc.integrationtests.CrateJDBCMetaDataIntegrationTest STANDARD_OUT
    No need to download crate. Already downloaded VERSION[0.57.6] to: /tmp/crate.testing/downloads/crate-0.57.6.tar.gz
    Starting crate server process...
Picked up _JAVA_OPTIONS: -Xmx2048m -Xms512m
[2022-02-20 08:31:39,860][WARN ][bootstrap                ] unable to install syscall filter: seccomp unavailable: your kernel is buggy and you should upgrade
[2022-02-20 08:31:40,070][INFO ][node                     ] [Monte Terza Grande] version[2.4.2], pid[4214], build[${build/NA]
[2022-02-20 08:31:40,070][INFO ][node                     ] [Monte Terza Grande] initializing ...
[2022-02-20 08:31:40,391][INFO ][io.crate.plugin          ] [Monte Terza Grande] plugins loaded: [crate-sigar] 
[2022-02-20 08:31:41,181][INFO ][plugins                  ] [Monte Terza Grande] modules [], plugins [blob, udc, crate-azure-discovery, crate-core, repository-hdfs, discovery-multicast, crate-admin-ui, cloud-aws, crate, discovery-srv, admin-ui], sites [crate-admin-ui]
[2022-02-20 08:31:41,204][INFO ][env                      ] [Monte Terza Grande] using [1] data paths, mounts [[/ (overlay)]], net usable_space [84.5tb], net total_space [90.9tb], spins? [possibly], types [overlay]
[2022-02-20 08:31:41,204][INFO ][env                      ] [Monte Terza Grande] heap size [1.9gb], compressed ordinary object pointers [true]
[2022-02-20 08:31:41,204][WARN ][env                      ] [Monte Terza Grande] max file descriptors [30000] for elasticsearch process likely too low, consider increasing to at least [65536]
[2022-02-20 08:31:41,421][INFO ][http                     ] [Monte Terza Grande] Using [io.crate.http.netty.CrateNettyHttpServerTransport] as http transport, overridden by [crate]
[2022-02-20 08:31:41,432][INFO ][plugin.hadoop.hdfs       ] Loaded Hadoop [Unknown] libraries from file:/tmp/crate.testing/working/crate-0.57.6_3af6d87f-7214-4f60-aa2a-7275bec205f5/plugins/elasticsearch-repository-hdfs/
[2022-02-20 08:31:41,483][INFO ][io.crate.module          ] [Monte Terza Grande] configuring crate. version: 0.57.6
[2022-02-20 08:31:41,712][INFO ][io.crate.module          ] [Monte Terza Grande] configuring crate. version: 0.57.6
[2022-02-20 08:31:43,540][INFO ][io.crate.operation.reference.sys.cluster.ClusterSettingsExpression$ApplySettings] updating [psql.port] from [5432-5532] to [5480]
[2022-02-20 08:31:44,367][INFO ][io.crate.rest            ] [Monte Terza Grande] Elasticsearch HTTP REST API not enabled
[2022-02-20 08:31:44,438][INFO ][node                     ] [Monte Terza Grande] initialized
[2022-02-20 08:31:44,439][INFO ][node                     ] [Monte Terza Grande] starting ...
[2022-02-20 08:31:44,602][INFO ][psql                     ] [Monte Terza Grande] publish_address {127.0.0.1:5480}, bound_addresses {127.0.0.1:5480}
[2022-02-20 08:31:44,602][INFO ][io.crate.blob.BlobService] [Monte Terza Grande] BlobService.doStart() io.crate.blob.BlobService@70ef6a50
[2022-02-20 08:31:44,657][INFO ][http                     ] [Monte Terza Grande] publish_address {127.0.0.1:4506}, bound_addresses {127.0.0.1:4506}
[2022-02-20 08:31:44,703][INFO ][transport                ] [Monte Terza Grande] publish_address {127.0.0.1:4254}, bound_addresses {127.0.0.1:4254}
[2022-02-20 08:31:44,710][INFO ][discovery                ] [Monte Terza Grande] TestingCluster/oBusC8w8S8GvHZHtGaYnUw
[2022-02-20 08:31:44,920][DEBUG][action.admin.indices.create] [Monte Terza Grande] no known master node, scheduling a retry
[2022-02-20 08:31:47,734][INFO ][cluster.service          ] [Monte Terza Grande] new_master {Monte Terza Grande}{oBusC8w8S8GvHZHtGaYnUw}{127.0.0.1}{127.0.0.1:4254}{http_address=127.0.0.1:4506, info.extended.type=sigar}, reason: zen-disco-join(elected_as_master, [0] joins received)
[2022-02-20 08:31:47,739][INFO ][node                     ] [Monte Terza Grande] started
[2022-02-20 08:31:49,155][INFO ][gateway                  ] [Monte Terza Grande] recovered [0] indices into cluster_state
[2022-02-20 08:31:49,320][INFO ][cluster.metadata         ] [Monte Terza Grande] [test.cluster] creating index, cause [api], templates [], shards [4]/[1], mappings [default]
[2022-02-20 08:31:51,604][INFO ][cluster.routing.allocation] [Monte Terza Grande] Cluster health status changed from [RED] to [YELLOW] (reason: [shards started [[test.cluster][0], [test.cluster][2], [test.cluster][1], [test.cluster][3]] ...]).
[2022-02-20 08:31:52,054][INFO ][cluster.metadata         ] [Monte Terza Grande] [names] creating index, cause [api], templates [], shards [4]/[1], mappings [default]
[2022-02-20 08:31:56,583][INFO ][cluster.routing.allocation] [Monte Terza Grande] Cluster health status changed from [RED] to [YELLOW] (reason: [shards started [[names][1], [names][3], [names][0], [names][2]] ...]).
[2022-02-20 08:31:59,099][INFO ][cluster.metadata         ] [Monte Terza Grande] [my_schema.names] creating index, cause [api], templates [], shards [4]/[1], mappings [default]
[2022-02-20 08:32:02,622][INFO ][cluster.routing.allocation] [Monte Terza Grande] Cluster health status changed from [RED] to [YELLOW] (reason: [shards started [[my_schema.names][3], [my_schema.names][2], [my_schema.names][1], [my_schema.names][0]] ...]).
[2022-02-20 08:32:06,833][INFO ][cluster.metadata         ] [Monte Terza Grande] [t_multi_pks] creating index, cause [api], templates [], shards [4]/[1], mappings [default]
[2022-02-20 08:32:09,349][INFO ][cluster.routing.allocation] [Monte Terza Grande] Cluster health status changed from [RED] to [YELLOW] (reason: [shards started [[t_multi_pks][2], [t_multi_pks][1], [t_multi_pks][0], [t_multi_pks][3]] ...]).
    Stopping crate server process...
[2022-02-20 08:32:12,518][INFO ][node                     ] [Monte Terza Grande] stopping ...
[2022-02-20 08:32:12,602][INFO ][node                     ] [Monte Terza Grande] stopped
[2022-02-20 08:32:12,603][INFO ][node                     ] [Monte Terza Grande] closing ...
[2022-02-20 08:32:12,613][INFO ][node                     ] [Monte Terza Grande] closed

io.crate.client.jdbc.integrationtests.CrateJDBCTypesTest STANDARD_OUT
    No need to download crate. Already downloaded VERSION[0.57.6] to: /tmp/crate.testing/downloads/crate-0.57.6.tar.gz
    Starting crate server process...
Picked up _JAVA_OPTIONS: -Xmx2048m -Xms512m
[2022-02-20 08:32:13,827][WARN ][bootstrap                ] unable to install syscall filter: seccomp unavailable: your kernel is buggy and you should upgrade
[2022-02-20 08:32:14,008][INFO ][node                     ] [Mont du Grand Capelet] version[2.4.2], pid[4604], build[${build/NA]
[2022-02-20 08:32:14,008][INFO ][node                     ] [Mont du Grand Capelet] initializing ...
[2022-02-20 08:32:14,244][INFO ][io.crate.plugin          ] [Mont du Grand Capelet] plugins loaded: [crate-sigar] 
[2022-02-20 08:32:15,248][INFO ][plugins                  ] [Mont du Grand Capelet] modules [], plugins [blob, udc, crate-azure-discovery, crate-core, repository-hdfs, discovery-multicast, crate-admin-ui, cloud-aws, crate, discovery-srv, admin-ui], sites [crate-admin-ui]
[2022-02-20 08:32:15,274][INFO ][env                      ] [Mont du Grand Capelet] using [1] data paths, mounts [[/ (overlay)]], net usable_space [84.5tb], net total_space [90.9tb], spins? [possibly], types [overlay]
[2022-02-20 08:32:15,274][INFO ][env                      ] [Mont du Grand Capelet] heap size [1.9gb], compressed ordinary object pointers [true]
[2022-02-20 08:32:15,274][WARN ][env                      ] [Mont du Grand Capelet] max file descriptors [30000] for elasticsearch process likely too low, consider increasing to at least [65536]
[2022-02-20 08:32:15,557][INFO ][http                     ] [Mont du Grand Capelet] Using [io.crate.http.netty.CrateNettyHttpServerTransport] as http transport, overridden by [crate]
[2022-02-20 08:32:15,571][INFO ][plugin.hadoop.hdfs       ] Loaded Hadoop [Unknown] libraries from file:/tmp/crate.testing/working/crate-0.57.6_95044305-8d00-47aa-b1b9-5f68db1bc0f0/plugins/elasticsearch-repository-hdfs/
[2022-02-20 08:32:15,635][INFO ][io.crate.module          ] [Mont du Grand Capelet] configuring crate. version: 0.57.6
[2022-02-20 08:32:15,889][INFO ][io.crate.module          ] [Mont du Grand Capelet] configuring crate. version: 0.57.6
[2022-02-20 08:32:17,463][INFO ][io.crate.operation.reference.sys.cluster.ClusterSettingsExpression$ApplySettings] updating [psql.port] from [5432-5532] to [5436]
[2022-02-20 08:32:18,325][INFO ][io.crate.rest            ] [Mont du Grand Capelet] Elasticsearch HTTP REST API not enabled
[2022-02-20 08:32:18,413][INFO ][node                     ] [Mont du Grand Capelet] initialized
[2022-02-20 08:32:18,413][INFO ][node                     ] [Mont du Grand Capelet] starting ...
[2022-02-20 08:32:18,585][INFO ][psql                     ] [Mont du Grand Capelet] publish_address {127.0.0.1:5436}, bound_addresses {127.0.0.1:5436}
[2022-02-20 08:32:18,585][INFO ][io.crate.blob.BlobService] [Mont du Grand Capelet] BlobService.doStart() io.crate.blob.BlobService@6bd10d8a
[2022-02-20 08:32:18,643][INFO ][http                     ] [Mont du Grand Capelet] publish_address {127.0.0.1:4599}, bound_addresses {127.0.0.1:4599}
[2022-02-20 08:32:18,697][INFO ][transport                ] [Mont du Grand Capelet] publish_address {127.0.0.1:4367}, bound_addresses {127.0.0.1:4367}
[2022-02-20 08:32:18,704][INFO ][discovery                ] [Mont du Grand Capelet] TestingCluster/w5c2IVuuRE2qyLBwjf78Iw
[2022-02-20 08:32:19,033][DEBUG][action.admin.indices.create] [Mont du Grand Capelet] no known master node, scheduling a retry
[2022-02-20 08:32:21,735][INFO ][cluster.service          ] [Mont du Grand Capelet] new_master {Mont du Grand Capelet}{w5c2IVuuRE2qyLBwjf78Iw}{127.0.0.1}{127.0.0.1:4367}{http_address=127.0.0.1:4599, info.extended.type=sigar}, reason: zen-disco-join(elected_as_master, [0] joins received)
[2022-02-20 08:32:21,740][INFO ][node                     ] [Mont du Grand Capelet] started
[2022-02-20 08:32:22,415][INFO ][gateway                  ] [Mont du Grand Capelet] recovered [0] indices into cluster_state
[2022-02-20 08:32:22,651][INFO ][cluster.metadata         ] [Mont du Grand Capelet] [test] creating index, cause [api], templates [], shards [1]/[0], mappings [default]
[2022-02-20 08:32:24,937][INFO ][cluster.routing.allocation] [Mont du Grand Capelet] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[test][0]] ...]).
[2022-02-20 08:32:26,220][INFO ][cluster.metadata         ] [Mont du Grand Capelet] [arraytest] creating index, cause [api], templates [], shards [1]/[0], mappings [default]
[2022-02-20 08:32:29,014][INFO ][cluster.routing.allocation] [Mont du Grand Capelet] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[arraytest][0]] ...]).
[2022-02-20 08:32:29,782][INFO ][cluster.metadata         ] [Mont du Grand Capelet] [arraytest] update_mapping [default]
[2022-02-20 08:32:31,376][INFO ][cluster.metadata         ] [Mont du Grand Capelet] [test_obj] creating index, cause [api], templates [], shards [4]/[1], mappings [default]
[2022-02-20 08:32:33,192][INFO ][cluster.routing.allocation] [Mont du Grand Capelet] Cluster health status changed from [RED] to [YELLOW] (reason: [shards started [[test_obj][1], [test_obj][0], [test_obj][3], [test_obj][2]] ...]).
    Stopping crate server process...
[2022-02-20 08:32:35,702][INFO ][node                     ] [Mont du Grand Capelet] stopping ...
[2022-02-20 08:32:37,928][INFO ][node                     ] [Mont du Grand Capelet] stopped
[2022-02-20 08:32:37,929][INFO ][node                     ] [Mont du Grand Capelet] closing ...
[2022-02-20 08:32:37,947][INFO ][node                     ] [Mont du Grand Capelet] closed

io.crate.client.jdbc.integrationtests.CrateJDBCUnsupportedFeaturesTest STANDARD_OUT
    No need to download crate. Already downloaded VERSION[0.57.6] to: /tmp/crate.testing/downloads/crate-0.57.6.tar.gz
    Starting crate server process...
Picked up _JAVA_OPTIONS: -Xmx2048m -Xms512m
[2022-02-20 08:32:39,158][WARN ][bootstrap                ] unable to install syscall filter: seccomp unavailable: your kernel is buggy and you should upgrade
[2022-02-20 08:32:39,376][INFO ][node                     ] [Hochschober] version[2.4.2], pid[5042], build[${build/NA]
[2022-02-20 08:32:39,376][INFO ][node                     ] [Hochschober] initializing ...
[2022-02-20 08:32:39,663][INFO ][io.crate.plugin          ] [Hochschober] plugins loaded: [crate-sigar] 
[2022-02-20 08:32:40,504][INFO ][plugins                  ] [Hochschober] modules [], plugins [blob, udc, crate-azure-discovery, crate-core, repository-hdfs, discovery-multicast, crate-admin-ui, cloud-aws, crate, discovery-srv, admin-ui], sites [crate-admin-ui]
[2022-02-20 08:32:40,524][INFO ][env                      ] [Hochschober] using [1] data paths, mounts [[/ (overlay)]], net usable_space [84.5tb], net total_space [90.9tb], spins? [possibly], types [overlay]
[2022-02-20 08:32:40,524][INFO ][env                      ] [Hochschober] heap size [1.9gb], compressed ordinary object pointers [true]
[2022-02-20 08:32:40,524][WARN ][env                      ] [Hochschober] max file descriptors [30000] for elasticsearch process likely too low, consider increasing to at least [65536]
[2022-02-20 08:32:40,744][INFO ][http                     ] [Hochschober] Using [io.crate.http.netty.CrateNettyHttpServerTransport] as http transport, overridden by [crate]
[2022-02-20 08:32:40,756][INFO ][plugin.hadoop.hdfs       ] Loaded Hadoop [Unknown] libraries from file:/tmp/crate.testing/working/crate-0.57.6_827c2966-be33-4680-b663-6985914693b4/plugins/elasticsearch-repository-hdfs/
[2022-02-20 08:32:40,807][INFO ][io.crate.module          ] [Hochschober] configuring crate. version: 0.57.6
[2022-02-20 08:32:41,030][INFO ][io.crate.module          ] [Hochschober] configuring crate. version: 0.57.6
[2022-02-20 08:32:42,774][INFO ][io.crate.operation.reference.sys.cluster.ClusterSettingsExpression$ApplySettings] updating [psql.port] from [5432-5532] to [5509]
[2022-02-20 08:32:43,578][INFO ][io.crate.rest            ] [Hochschober] Elasticsearch HTTP REST API not enabled
[2022-02-20 08:32:43,646][INFO ][node                     ] [Hochschober] initialized
[2022-02-20 08:32:43,646][INFO ][node                     ] [Hochschober] starting ...
[2022-02-20 08:32:43,846][INFO ][psql                     ] [Hochschober] publish_address {127.0.0.1:5509}, bound_addresses {127.0.0.1:5509}
[2022-02-20 08:32:43,846][INFO ][io.crate.blob.BlobService] [Hochschober] BlobService.doStart() io.crate.blob.BlobService@4cc6129c
[2022-02-20 08:32:43,906][INFO ][http                     ] [Hochschober] publish_address {127.0.0.1:4533}, bound_addresses {127.0.0.1:4533}
[2022-02-20 08:32:43,958][INFO ][transport                ] [Hochschober] publish_address {127.0.0.1:4324}, bound_addresses {127.0.0.1:4324}
[2022-02-20 08:32:43,966][INFO ][discovery                ] [Hochschober] TestingCluster/56iKkg1uQtGkzBtlypq-GQ
    Stopping crate server process...
[2022-02-20 08:32:44,261][INFO ][node                     ] [Hochschober] stopping ...
[2022-02-20 08:32:44,326][INFO ][node                     ] [Hochschober] stopped
[2022-02-20 08:32:44,327][INFO ][node                     ] [Hochschober] closing ...
[2022-02-20 08:32:44,334][INFO ][node                     ] [Hochschober] closed

92 tests completed, 1 failed, 7 skipped
:test FAILED

FAILURE: Build failed with an exception.

* What went wrong:
Execution failed for task ':test'.
> There were failing tests. See the report at: file:///home/travis/build/failed/crate/crate-jdbc/build/reports/tests/test/index.html

* Try:
Run with --info or --debug option to get more log output.

* Exception is:
org.gradle.api.tasks.TaskExecutionException: Execution failed for task ':test'.
	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:100)
	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:70)
	at org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter.execute(SkipUpToDateTaskExecuter.java:64)
	at org.gradle.api.internal.tasks.execution.ResolveTaskOutputCachingStateExecuter.execute(ResolveTaskOutputCachingStateExecuter.java:54)
	at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:58)
	at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:88)
	at org.gradle.api.internal.tasks.execution.ResolveTaskArtifactStateTaskExecuter.execute(ResolveTaskArtifactStateTaskExecuter.java:52)
	at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:52)
	at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskExecuter.java:54)
	at org.gradle.api.internal.tasks.execution.ExecuteAtMostOnceTaskExecuter.execute(ExecuteAtMostOnceTaskExecuter.java:43)
	at org.gradle.api.internal.tasks.execution.CatchExceptionTaskExecuter.execute(CatchExceptionTaskExecuter.java:34)
	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker$1.run(DefaultTaskGraphExecuter.java:242)
	at org.gradle.internal.progress.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:317)
	at org.gradle.internal.progress.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:309)
	at org.gradle.internal.progress.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:185)
	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:95)
	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:235)
	at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:224)
	at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$TaskExecutorWorker.processTask(DefaultTaskPlanExecutor.java:121)
	at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$TaskExecutorWorker.access$200(DefaultTaskPlanExecutor.java:77)
	at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$TaskExecutorWorker$1.execute(DefaultTaskPlanExecutor.java:102)
	at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$TaskExecutorWorker$1.execute(DefaultTaskPlanExecutor.java:96)
	at org.gradle.execution.taskgraph.DefaultTaskExecutionPlan.execute(DefaultTaskExecutionPlan.java:612)
	at org.gradle.execution.taskgraph.DefaultTaskExecutionPlan.executeWithTask(DefaultTaskExecutionPlan.java:567)
	at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$TaskExecutorWorker.run(DefaultTaskPlanExecutor.java:96)
	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:63)
	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:46)
	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:55)
Caused by: org.gradle.api.GradleException: There were failing tests. See the report at: file:///home/travis/build/failed/crate/crate-jdbc/build/reports/tests/test/index.html
	at org.gradle.api.tasks.testing.Test.handleTestFailures(Test.java:1411)
	at org.gradle.api.tasks.testing.Test.executeTests(Test.java:702)
	at org.gradle.internal.reflect.JavaMethod.invoke(JavaMethod.java:73)
	at org.gradle.api.internal.project.taskfactory.DefaultTaskClassInfoStore$StandardTaskAction.doExecute(DefaultTaskClassInfoStore.java:141)
	at org.gradle.api.internal.project.taskfactory.DefaultTaskClassInfoStore$StandardTaskAction.execute(DefaultTaskClassInfoStore.java:134)
	at org.gradle.api.internal.project.taskfactory.DefaultTaskClassInfoStore$StandardTaskAction.execute(DefaultTaskClassInfoStore.java:121)
	at org.gradle.api.internal.AbstractTask$TaskActionWrapper.execute(AbstractTask.java:711)
	at org.gradle.api.internal.AbstractTask$TaskActionWrapper.execute(AbstractTask.java:694)
	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter$1.run(ExecuteActionsTaskExecuter.java:122)
	at org.gradle.internal.progress.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:317)
	at org.gradle.internal.progress.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:309)
	at org.gradle.internal.progress.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:185)
	at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:95)
	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeAction(ExecuteActionsTaskExecuter.java:111)
	at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:92)
	... 27 more


BUILD FAILED in 2m 45s
8 actionable tasks: 5 executed, 3 up-to-date

travis_time:end:313c4430:start=1645345799172296645,finish=1645345965742003239,duration=166569706594[0K
[31;1mThe command "./gradlew test -s" exited with 1.[0m
travis_fold:start:cache.2[0Kstore build cache
travis_time:start:213e1b73[0K
travis_time:end:213e1b73:start=1645345965756574031,finish=1645345965767979102,duration=11405071[0Ktravis_time:start:023fb34a[0K/home/travis/.casher/bin/casher:190: warning: Insecure world writable dir /usr/local/clang-5.0.0/bin in PATH, mode 040777
[32;1mchanges detected, packing new archive[0m
/home/travis/.casher/bin/casher:213:in `cache_archive_name': undefined method `[]' for nil:NilClass (NoMethodError)
	from /home/travis/.casher/bin/casher:143:in `push'
	from /home/travis/.casher/bin/casher:53:in `block in run'
	from /home/travis/.rvm/rubies/ruby-2.2.7/lib/ruby/2.2.0/timeout.rb:88:in `block in timeout'
	from /home/travis/.rvm/rubies/ruby-2.2.7/lib/ruby/2.2.0/timeout.rb:32:in `block in catch'
	from /home/travis/.rvm/rubies/ruby-2.2.7/lib/ruby/2.2.0/timeout.rb:32:in `catch'
	from /home/travis/.rvm/rubies/ruby-2.2.7/lib/ruby/2.2.0/timeout.rb:32:in `catch'
	from /home/travis/.rvm/rubies/ruby-2.2.7/lib/ruby/2.2.0/timeout.rb:103:in `timeout'
	from /home/travis/.casher/bin/casher:53:in `run'
	from /home/travis/.casher/bin/casher:263:in `<main>'

travis_time:end:023fb34a:start=1645345965781231625,finish=1645345967709544629,duration=1928313004[0Ktravis_fold:end:cache.2[0K
Done. Your build exited with 1.
